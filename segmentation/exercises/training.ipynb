{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify Network Architecture\n",
    "## mknet.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating U-Net layer 0\n",
      "f_in: (1, 1, 84, 268, 268)\n",
      "number of variables added: 4236, new total: 4236\n",
      "    Creating U-Net layer 1\n",
      "    f_in: (1, 12, 80, 88, 88)\n",
      "    number of variables added: 116760, new total: 120996\n",
      "        Creating U-Net layer 2\n",
      "        f_in: (1, 60, 76, 28, 28)\n",
      "        number of variables added: 2916600, new total: 3037596\n",
      "            Creating U-Net layer 3\n",
      "            f_in: (1, 300, 24, 8, 8)\n",
      "            bottom layer\n",
      "            f_out: (1, 1500, 20, 4, 4)\n",
      "            number of variables added: 72903000, new total: 75940596\n",
      "        g_out: (1, 1500, 20, 4, 4)\n",
      "        g_out_upsampled: (1, 300, 60, 12, 12)\n",
      "        f_left_cropped: (1, 300, 60, 12, 12)\n",
      "        f_right: (1, 600, 60, 12, 12)\n",
      "        f_out: (1, 300, 56, 8, 8)\n",
      "        number of variables added: 19440900, new total: 95381496\n",
      "    g_out: (1, 300, 56, 8, 8)\n",
      "    g_out_upsampled: (1, 60, 56, 24, 24)\n",
      "    f_left_cropped: (1, 60, 56, 24, 24)\n",
      "    f_right: (1, 120, 56, 24, 24)\n",
      "    f_out: (1, 60, 52, 20, 20)\n",
      "    number of variables added: 453780, new total: 95835276\n",
      "g_out: (1, 60, 52, 20, 20)\n",
      "g_out_upsampled: (1, 12, 52, 60, 60)\n",
      "f_left_cropped: (1, 12, 52, 60, 60)\n",
      "f_right: (1, 24, 52, 60, 60)\n",
      "f_out: (1, 12, 48, 56, 56)\n",
      "number of variables added: 18180, new total: 95853456\n",
      "WARNING:tensorflow:From /groups/funke/home/ecksteinn/miniconda2/envs/segmentation/lib/python3.7/site-packages/tensorflow/python/ops/losses/losses_impl.py:667: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "input shape : (84, 268, 268)\n",
      "output shape: [48, 56, 56]\n",
      "Creating U-Net layer 0\n",
      "f_in: (1, 1, 114, 646, 646)\n",
      "number of variables added: 4236, new total: 4236\n",
      "    Creating U-Net layer 1\n",
      "    f_in: (1, 12, 110, 214, 214)\n",
      "    number of variables added: 116760, new total: 120996\n",
      "        Creating U-Net layer 2\n",
      "        f_in: (1, 60, 106, 70, 70)\n",
      "        number of variables added: 2916600, new total: 3037596\n",
      "            Creating U-Net layer 3\n",
      "            f_in: (1, 300, 34, 22, 22)\n",
      "            bottom layer\n",
      "            f_out: (1, 1500, 30, 18, 18)\n",
      "            number of variables added: 72903000, new total: 75940596\n",
      "        g_out: (1, 1500, 30, 18, 18)\n",
      "        g_out_upsampled: (1, 300, 90, 54, 54)\n",
      "        f_left_cropped: (1, 300, 90, 54, 54)\n",
      "        f_right: (1, 600, 90, 54, 54)\n",
      "        f_out: (1, 300, 86, 50, 50)\n",
      "        number of variables added: 19440900, new total: 95381496\n",
      "    g_out: (1, 300, 86, 50, 50)\n",
      "    g_out_upsampled: (1, 60, 86, 150, 150)\n",
      "    f_left_cropped: (1, 60, 86, 150, 150)\n",
      "    f_right: (1, 120, 86, 150, 150)\n",
      "    f_out: (1, 60, 82, 146, 146)\n",
      "    number of variables added: 453780, new total: 95835276\n",
      "g_out: (1, 60, 82, 146, 146)\n",
      "g_out_upsampled: (1, 12, 82, 438, 438)\n",
      "f_left_cropped: (1, 12, 82, 438, 438)\n",
      "f_right: (1, 24, 82, 438, 438)\n",
      "f_out: (1, 12, 78, 434, 434)\n",
      "number of variables added: 18180, new total: 95853456\n",
      "input shape : (114, 646, 646)\n",
      "output shape: [78, 434, 434]\n"
     ]
    }
   ],
   "source": [
    "from funlib.learn.tensorflow import models\n",
    "import malis\n",
    "import tensorflow as tf\n",
    "import json\n",
    "\n",
    "def create_network(input_shape, name):\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    with tf.variable_scope('setup0'):\n",
    "\n",
    "        raw = tf.placeholder(tf.float32, shape=input_shape)\n",
    "        raw_batched = tf.reshape(raw, (1, 1) + input_shape)\n",
    "\n",
    "        unet, _, _ = models.unet(\n",
    "                raw_batched,\n",
    "                12,\n",
    "                5,\n",
    "                [[1,3,3],[1,3,3],[3,3,3]])\n",
    "\n",
    "        affs_batched, _ = models.conv_pass(\n",
    "            unet,\n",
    "            kernel_sizes=[1],\n",
    "            num_fmaps=3,\n",
    "            activation='sigmoid',\n",
    "            name='affs')\n",
    "\n",
    "        output_shape_batched = affs_batched.get_shape().as_list()\n",
    "        output_shape = output_shape_batched[1:] # strip the batch dimension\n",
    "\n",
    "        affs = tf.reshape(affs_batched, output_shape)\n",
    "\n",
    "        gt_affs = tf.placeholder(tf.float32, shape=output_shape)\n",
    "        affs_loss_weights = tf.placeholder(tf.float32, shape=output_shape)\n",
    "\n",
    "        loss = tf.losses.mean_squared_error(\n",
    "            gt_affs,\n",
    "            affs,\n",
    "            affs_loss_weights)\n",
    "\n",
    "        summary = tf.summary.scalar('setup0', loss)\n",
    "\n",
    "        opt = tf.train.AdamOptimizer(\n",
    "            learning_rate=0.5e-4,\n",
    "            beta1=0.95,\n",
    "            beta2=0.999,\n",
    "            epsilon=1e-8)\n",
    "        optimizer = opt.minimize(loss)\n",
    "\n",
    "        output_shape = output_shape[1:]\n",
    "        print(\"input shape : %s\"%(input_shape,))\n",
    "        print(\"output shape: %s\"%(output_shape,))\n",
    "\n",
    "        tf.train.export_meta_graph(filename=name + '.meta')\n",
    "\n",
    "        config = {\n",
    "            'raw': raw.name,\n",
    "            'affs': affs.name,\n",
    "            'gt_affs': gt_affs.name,\n",
    "            'affs_loss_weights': affs_loss_weights.name,\n",
    "            'loss': loss.name,\n",
    "            'optimizer': optimizer.name,\n",
    "            'input_shape': input_shape,\n",
    "            'output_shape': output_shape,\n",
    "            'summary': summary.name\n",
    "        }\n",
    "\n",
    "        config['outputs'] = {'affs': {\"out_dims\": 3, \"out_dtype\": \"uint8\"}}\n",
    "\n",
    "        with open(name + '.json', 'w') as f:\n",
    "            json.dump(config, f)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    z=18\n",
    "    xy=162\n",
    "\n",
    "    create_network((84, 268, 268), 'train_net')\n",
    "    create_network((96+z, 484+xy, 484+xy), 'config')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gunpowder.tensorflow.local_server:Creating local tensorflow server\n",
      "INFO:gunpowder.tensorflow.local_server:Server running at b'grpc://localhost:36769'\n",
      "INFO:gunpowder.tensorflow.nodes.train:Initializing tf session, connecting to b'grpc://localhost:36769'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTEXT:  (960, 112, 112)\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gunpowder.tensorflow.nodes.train:Reading meta-graph...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /groups/funke/home/ecksteinn/miniconda2/envs/segmentation/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /groups/funke/home/ecksteinn/miniconda2/envs/segmentation/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "INFO:gunpowder.tensorflow.nodes.train:No checkpoint found\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "#import multiprocessing\n",
    "#multiprocessing.set_start_method('forkserver')\n",
    "import sys\n",
    "from gunpowder import *\n",
    "from gunpowder.tensorflow import *\n",
    "import os\n",
    "import math\n",
    "import json\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "data_dir = '../data'\n",
    "\n",
    "samples = [\n",
    "    'sample_A',\n",
    "    'sample_B',\n",
    "    'sample_C'\n",
    "]\n",
    "\n",
    "neighborhood = [[-1, 0, 0], [0, -1, 0], [0, 0, -1]]\n",
    "\n",
    "def train_until(max_iteration):\n",
    "\n",
    "    if tf.train.latest_checkpoint('.'):\n",
    "        trained_until = int(tf.train.latest_checkpoint('.').split('_')[-1])\n",
    "    else:\n",
    "        trained_until = 0\n",
    "    if trained_until >= max_iteration:\n",
    "        return\n",
    "\n",
    "    with open('train_net.json', 'r') as f:\n",
    "        config = json.load(f)\n",
    "\n",
    "    raw = ArrayKey('RAW')\n",
    "    labels = ArrayKey('GT_LABELS')\n",
    "    labels_mask = ArrayKey('GT_LABELS_MASK')\n",
    "    affs = ArrayKey('PREDICTED_AFFS')\n",
    "    gt = ArrayKey('GT_AFFINITIES')\n",
    "    gt_mask = ArrayKey('GT_AFFINITIES_MASK')\n",
    "    gt_scale = ArrayKey('GT_AFFINITIES_SCALE')\n",
    "    affs_gradient = ArrayKey('AFFS_GRADIENT')\n",
    "\n",
    "    voxel_size = Coordinate((40, 4, 4))\n",
    "    input_size = Coordinate(config['input_shape'])*voxel_size\n",
    "    output_size = Coordinate(config['output_shape'])*voxel_size\n",
    "    context = output_size/2\n",
    "    print('CONTEXT: ', context)\n",
    "\n",
    "    request = BatchRequest()\n",
    "    request.add(raw, input_size)\n",
    "    request.add(labels, output_size)\n",
    "    request.add(labels_mask, output_size)\n",
    "    request.add(gt, output_size)\n",
    "    request.add(gt_mask, output_size)\n",
    "    request.add(gt_scale, output_size)\n",
    "\n",
    "    snapshot_request = BatchRequest({\n",
    "        affs: request[gt],\n",
    "        affs_gradient: request[gt]\n",
    "    })\n",
    "\n",
    "    data_sources = tuple(\n",
    "        N5Source(\n",
    "            os.path.join(data_dir, sample + '.n5'),\n",
    "            datasets = {\n",
    "                raw: 'volumes/raw',\n",
    "                labels: 'volumes/labels/neuron_ids',\n",
    "                labels_mask: 'volumes/labels/mask',\n",
    "            },\n",
    "            array_specs = {\n",
    "                raw: ArraySpec(interpolatable=True),\n",
    "                labels: ArraySpec(interpolatable=False),\n",
    "                labels_mask: ArraySpec(interpolatable=False)\n",
    "            }\n",
    "        ) +\n",
    "        Normalize(raw) +\n",
    "        Pad(labels, context) +\n",
    "        Pad(labels_mask, context) +\n",
    "        RandomLocation() +\n",
    "        Reject(mask=labels_mask)\n",
    "        for sample in samples\n",
    "    )\n",
    "\n",
    "\n",
    "    train_pipeline = (\n",
    "        data_sources +\n",
    "        RandomProvider() +\n",
    "        ElasticAugment(\n",
    "            control_point_spacing=[4,40,40],\n",
    "            jitter_sigma=[0,2,2],\n",
    "            rotation_interval=[0,math.pi/2.0],\n",
    "            prob_slip=0.05,\n",
    "            prob_shift=0.05,\n",
    "            max_misalign=10,\n",
    "            subsample=8) +\n",
    "        SimpleAugment(transpose_only=[1, 2]) +\n",
    "        IntensityAugment(raw, 0.9, 1.1, -0.1, 0.1, z_section_wise=True) +\n",
    "        GrowBoundary(labels, labels_mask, steps=1, only_xy=True) +\n",
    "        AddAffinities(\n",
    "            neighborhood,\n",
    "            labels=labels,\n",
    "            affinities=gt,\n",
    "            labels_mask=labels_mask,\n",
    "            affinities_mask=gt_mask) +\n",
    "        BalanceLabels(\n",
    "            gt,\n",
    "            gt_scale,\n",
    "            gt_mask) +\n",
    "        DefectAugment(\n",
    "            raw,\n",
    "            prob_missing=0.03,\n",
    "            prob_low_contrast=0.01,\n",
    "            contrast_scale=0.5,\n",
    "            axis=0) +\n",
    "        IntensityScaleShift(raw, 2,-1) +\n",
    "        PreCache(cache_size=40,\n",
    "                 num_workers=10) +\n",
    "        Train(\n",
    "            'train_net',\n",
    "            optimizer=config['optimizer'],\n",
    "            loss=config['loss'],\n",
    "            inputs={\n",
    "                config['raw']: raw,\n",
    "                config['gt_affs']: gt,\n",
    "                config['affs_loss_weights']: gt_scale,\n",
    "            },\n",
    "            outputs={\n",
    "                config['affs']: affs\n",
    "            },\n",
    "            gradients={\n",
    "                config['affs']: affs_gradient\n",
    "            },\n",
    "            summary=config['summary'],\n",
    "            log_dir='log',\n",
    "            save_every=10000) +\n",
    "        IntensityScaleShift(raw, 0.5, 0.5) +\n",
    "        Snapshot({\n",
    "                raw: 'volumes/raw',\n",
    "                labels: 'volumes/labels/neuron_ids',\n",
    "                gt: 'volumes/gt_affinities',\n",
    "                affs: 'volumes/pred_affinities',\n",
    "                gt_mask: 'volumes/labels/gt_mask',\n",
    "                labels_mask: 'volumes/labels/mask',\n",
    "                affs_gradient: 'volumes/affs_gradient'\n",
    "            },\n",
    "            dataset_dtypes={\n",
    "                labels: np.uint64\n",
    "            },\n",
    "            every=1000,\n",
    "            output_filename='batch_{iteration}.hdf',\n",
    "            additional_request=snapshot_request) +\n",
    "        PrintProfilingStats(every=10)\n",
    "    )\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    with build(train_pipeline) as b:\n",
    "        for i in range(max_iteration - trained_until):\n",
    "            b.request_batch(request)\n",
    "    print(\"Training finished\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    iteration = 500000\n",
    "    train_until(iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
